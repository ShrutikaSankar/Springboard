{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load python packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Path\n",
    "os.getcwd()\n",
    "basepath = os.getcwd()\n",
    "print(basepath)\n",
    "lis_dir = os.listdir()\n",
    "print(lis_dir)\n",
    "\n",
    "# Create the list of file names: filenames\n",
    "\"\"\"\n",
    "The Below file which I used was too big to upload to github so a saved version of a cleaned data set is used\n",
    "file ='vehicles.csv'\n",
    "\n",
    "\"\"\"\n",
    "file = 'vehicles.csv'\n",
    "sub_file = 'data'\n",
    "file_name_path = os.path.join(basepath, sub_file, file)\n",
    "auto_data= pd.read_csv(file_name_path)\n",
    "\n",
    "\n",
    "\n",
    "#Open data\n",
    "auto_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the columns names of the entire dataframe\n",
    "print(auto_data.columns)\n",
    "# Review the column names, null value counts and data types of your data frame.\n",
    "auto_data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables drop\n",
    "#url   - We will not be using urls in this study, \n",
    "#region_url   - We have already have region\n",
    "#image_url #We do not need photos at the moment \n",
    "#county    #there is no non 0 data points there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data= auto_data.drop(columns=['url', 'region_url', 'image_url', 'county'])\n",
    "print(auto_data.columns)\n",
    "\n",
    "\n",
    "#known main points for an auto sale condition, age, mileage, type of car if the cars don't have it we don't want it\n",
    "auto_data = auto_data[auto_data['condition'].notna()]  #if condition is unknown from wreck to clean we don't need itnew_file = 'autosub.csv'\n",
    "auto_data = auto_data[auto_data['year'].notna()]  #if we don't know model year we don't know age\n",
    "auto_data = auto_data[auto_data['odometer'].notna()]  #odometer measures mileage\n",
    "auto_data = auto_data[auto_data['model'].notna()]  #we don't know the model car we don't know the car\n",
    "auto_data = auto_data[auto_data['price'].notna()]  #we don't know the model car we don't know the car\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming Rules: Common Sense\n",
    "\n",
    "* Cylinders can be numeric\n",
    "* Manufacturer names should be consistent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn Cylinder column from string to numeric\n",
    "auto_data['cyl'] = auto_data['cylinders']\n",
    "auto_data['cyl'] = auto_data['cyl'].replace('other', np.nan)\n",
    "auto_data['cyl'] = auto_data['cyl'].astype(str).str[0:2]\n",
    "auto_data['cyl'] = auto_data['cyl'].str.strip()\n",
    "auto_data['cyl'] = auto_data['cyl'].replace('na', np.nan)\n",
    "auto_data['cyl'] = pd.to_numeric(auto_data['cyl'])\n",
    "auto_data['cylinders']= auto_data['cyl']\n",
    "auto_data= auto_data.drop(['cyl'], axis=1)\n",
    "\n",
    "#plot Cylinders\n",
    "plt.title(\"Numerical Cylinders\")\n",
    "hist = auto_data.cylinders.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neaten Strings\n",
    "#Fix manufacturer\n",
    "auto_data['manufacturer']= auto_data['manufacturer'].str.title().str.strip()\n",
    "auto_data['manufacturer']= auto_data['manufacturer'].str.replace(' ', '-')\n",
    "auto_data['manufacturer']= auto_data['manufacturer'].str.replace('rover' , 'land-rover') \n",
    "auto_data['manufacturer']= auto_data['manufacturer'].str.replace('land-land' , 'land')\n",
    "auto_data['manufacturer']= auto_data['manufacturer'].str.replace('porche', 'porsche')\n",
    "auto_data['manufacturer']= auto_data['manufacturer'].str.replace('---', '-')\n",
    "auto_data['manufacturer']= auto_data['manufacturer'].str.replace('--', '-')\n",
    "\n",
    "\n",
    "#Fix model\n",
    "auto_data['model']= auto_data['model'].str.title().str.strip()\n",
    "auto_data['model']= auto_data['model'].str.replace(' ', '-')\n",
    "auto_data['model']= auto_data['model'].str.replace('---', '-')\n",
    "auto_data['model']= auto_data['model'].str.replace('--', '-')\n",
    "\n",
    "#States are state codes\n",
    "auto_data['state'] = auto_data['state'].str.upper().str.strip()\n",
    "\n",
    "#If no paint color is given its unlisted and its unrealistc to try to impute it\n",
    "auto_data['paint_color'] = auto_data['paint_color'].fillna('unlisted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Rules: Common Sense\n",
    "\n",
    "* This study is not looking at over luxury cars/ antique cars  so \n",
    "    * None of the vehicles should have a price greater than half a million\n",
    "* This study is not looking at motorbikes or bus's \n",
    "    * we don't need harley davidson's or henessey's \n",
    "    *\n",
    "* This study is not looking at junker cars so we do not need vehicles \n",
    "    * of a value of a 100 dollars or less\n",
    "    * with over a half milion miles on them\n",
    "    * are of poor condition\n",
    "    * older than 10 years old (we are also not looking at antique cars\n",
    "    * that has a condition/title status is better than salvage \n",
    "* Cleaning Obvious errors:\n",
    "    * We are looking at used vehicles so the vehicles age should be greater than 1 year\n",
    "* We do not need to take into consideration vehicles older than 10 years old. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are dealing with a vehicle with over a half milion miles we have more problem\n",
    "auto_data = auto_data[(auto_data.odometer< 500000)] \n",
    "\n",
    "\n",
    "#This study is not look at eather junkers or antiques so \n",
    "#a: age has to be greater than 0 and less than 11 years\n",
    "#b: price has to be greater than $100 and less than half million\n",
    "#c: mileage has to be greater than or equal to 0 and less than half million\n",
    "\n",
    "auto_data['age'] = 2020-auto_data['year']\n",
    "auto_data = auto_data[(auto_data.age > 0) & (auto_data.age <= 10)]\n",
    "auto_data = auto_data[(auto_data.price >= 100) & (auto_data.price <= 500000)]\n",
    "auto_data = auto_data[(auto_data.odometer >= 0) & (auto_data.odometer <= 500000)]\n",
    "\n",
    "\n",
    "print(f\"Maximum price: $ {auto_data.price.max()} \\nMinimum price: $ {auto_data.price.min()} \")\n",
    "print(f\"Maximum age:  {auto_data.age.max()} years \\nMinimum age: {auto_data.age.min()} years \")\n",
    "print(f\"Maximum mileage: {auto_data.odometer.max()} \\nMinimum mileage: {auto_data.odometer.min()} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This study is not look junkers so condition and title_ status is better than salvage\n",
    "\n",
    "#See Status of Vehicles\n",
    "auto_data.title_status.value_counts().plot.bar()\n",
    "plt.title(\"Status of the vehicles\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See Condition of Vehicles\n",
    "\n",
    "auto_data.condition.value_counts().plot.bar()\n",
    "plt.title(\"Condition of the vehicles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This study is not look junkers so condition and title_ status is better than salvage\n",
    "\n",
    "idx1 = auto_data[auto_data[\"condition\"] == \"salvage\"].index\n",
    "\n",
    "for w in [\"salvage\",\"lien\",\"missing\",\"parts only\", 'rebuilt']:\n",
    "    idx2 = auto_data[auto_data[\"title_status\"] == w].index\n",
    "    idx1 = idx1.union(idx2)\n",
    "    \n",
    "auto_data.drop(idx1, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are not looking at motorbikes\n",
    "\n",
    "#Graph manufacturers\n",
    "\n",
    "auto_data.manufacturer.value_counts().plot.bar()\n",
    "plt.title(\"manufacturer of the vehicles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We are not looking at motorbikes\n",
    "auto_data = auto_data[auto_data.manufacturer != 'Harley-Davidson']\n",
    "auto_data = auto_data[auto_data.manufacturer != 'Hennessey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are not looking at buses\n",
    "#Graph type\n",
    "\n",
    "auto_data.type.value_counts().plot.bar()\n",
    "plt.title(\"type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are not looking at bus\n",
    "auto_data = auto_data[auto_data.type != 'bus']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because springboard has issues loading the data we create a new file\n",
    "\n",
    "z_file = 'auto_data_small.csv'\n",
    "z_file_name_path = os.path.join(basepath,sub_file, z_file)\n",
    "auto_data.to_csv(z_file_name_path, index=False)\n",
    "auto_data= pd.read_csv(file_name_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Make Model Model Year Key (Useful for later)\n",
    "auto_data['mmy_a']=auto_data['manufacturer']+' '+auto_data['model']+ ' '+ auto_data['year'].astype(int).astype(str)\n",
    "auto_data['mmy_a']=auto_data['mmy_a'].str.title()\n",
    "\n",
    "auto_data['mmy_a2']=auto_data['manufacturer']+' '+auto_data['model'].str.split(' ').str[0]+ ' '+ auto_data['year'].astype(int).astype(str)\n",
    "auto_data['mmy_a2']=auto_data['mmy_a2'].str.title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file = 'data'\n",
    "new_file = 'auto_data.csv'\n",
    "new_file_name_path = os.path.join(basepath,sub_file, new_file)\n",
    "auto_data.to_csv(new_file_name_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data\n",
    "We want data on what segment the vehicle is in as that might change the prices signicantly.\n",
    "We got this data from back4app an open data source.\n",
    "Website: https://www.back4app.com/database/back4app/car-make-model-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Categorical Data\n",
    "\n",
    "where = urllib.parse.quote_plus(\"\"\"\n",
    "{\n",
    "    \"Category\": {\n",
    "        \"$exists\": true\n",
    "    },\n",
    "    \"Year\": {\n",
    "        \"$gte\": 2010\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "url = 'https://parseapi.back4app.com/classes/Carmodels_Car_Model_List?limit=5000&where=%s' % where\n",
    "headers = {\n",
    "    'X-Parse-Application-Id': 'pozCwVISt2yfzi2wJjwo0JiMIJclsUAOchP4AuJO', # This is your app's application id\n",
    "    'X-Parse-REST-API-Key': 'Jtc8r91VIJOizG8ReKkaseZcugQjAx4vG1WKbJye' # This is your app's REST API key\n",
    "}\n",
    "data_json = json.loads(requests.get(url, headers=headers).content.decode('utf-8')) # Here you have the data that you need\n",
    "\n",
    "\n",
    "#import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above data was too nested a dictionaries listed in a list that was in a list that was a dictionary\n",
    "print(type(data_json))\n",
    "list_values = [ v for v in data_json.values() ] #undoes dictionary\n",
    "\n",
    "list_value_items = [y for x in list_values for y in x] # undoes list\n",
    "\n",
    "cat_data = pd.DataFrame.from_dict(list_value_items) #turns list of dictionaries into data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we don't need object id or created at\n",
    "cat_data= cat_data.drop(['objectId', 'createdAt'], axis=1)\n",
    "print(cat_data.head())\n",
    "cat_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neaten Strings\n",
    "cat_data['Make'] = cat_data['Make'].str.title().str.strip()\n",
    "cat_data['Make']= cat_data['Make'].str.replace(' ', '-') \n",
    "cat_data['Model'] = cat_data['Model'].str.title().str.strip()\n",
    "cat_data['Model']= cat_data['Model'].str.replace(' ', '-') \n",
    "\n",
    "#Create Key\n",
    "cat_data['mmy']=cat_data['Make']+' '+cat_data['Model'].str.strip()+ ' '+ cat_data['Year'].astype(int).astype(str)\n",
    "cat_data['mmy']= cat_data['mmy'].str.title()\n",
    "\n",
    "cat_data['mmy2']=cat_data['Make']+' '+cat_data['Model'].str.split(' ').str[0]+' '+ cat_data['Year'].astype(int).astype(str)\n",
    "cat_data['mmy2']= cat_data['mmy2'].str.title()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save Categorical Data\n",
    "sub_file = 'data'\n",
    "new_file = 'cat_data.csv'\n",
    "new_file_name_path = os.path.join(basepath,sub_file, new_file)\n",
    "cat_data.to_csv(new_file_name_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auto_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find MSRP data\n",
    "The original price of the car will heavily effect the older price. Let's bring that out.\n",
    "This data is a downloaded file from https://www.kaggle.com/CooperUnion/cardataset . This data was obtained by scraping edmunds (a car rating website) and twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of file names: filenames\n",
    "file ='msrp.csv'\n",
    "sub_file = 'data'\n",
    "file_name_path = os.path.join(basepath, sub_file, file)\n",
    "msrp_data= pd.read_csv(file_name_path)\n",
    "\n",
    "#Open data\n",
    "msrp_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the previous cleaning rules we eliminate vehicles with value of over half a million and an age over 10 years\n",
    "\n",
    "print(f\"Maximum MSRP: {msrp_data.MSRP.max()} \\nMinimum MSRP: {msrp_data.MSRP.min()} \")\n",
    "# If we are dealing with a vehicle with an MSRP over half milion miles we have more problem\n",
    "msrp_data = msrp_data[(msrp_data.MSRP< 500000 )] \n",
    "print(f\"Maximum Year: {msrp_data.Year.max()} \\nMinimum Year: {msrp_data.Year.min()} \")\n",
    "# We only need vehicles with over a 2009 MY\n",
    "msrp_data = msrp_data[(msrp_data.Year> 2009 )] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Strings/ Neaten Strings\n",
    "msrp_data['Make']= msrp_data['Make'].str.title().str.strip()\n",
    "msrp_data['Make']= msrp_data['Make'].str.replace(' ', '-')\n",
    "msrp_data['Model']= msrp_data['Model'].str.title().str.strip()\n",
    "msrp_data['Model']= msrp_data['Model'].str.replace(' ', '-')\n",
    "\n",
    "#Make Keys\n",
    "msrp_data['mmy']=msrp_data['Make']+' '+msrp_data['Model']+ ' '+ msrp_data['Year'].astype(int).astype(str)\n",
    "msrp_data['mmy']= msrp_data['mmy'].str.title()\n",
    "msrp_data['mmy2']=msrp_data['Make']+' '+msrp_data['Model'].str.split(' ').str[0]+ ' '+ msrp_data['Year'].astype(int).astype(str)\n",
    "msrp_data['mmy2']= msrp_data['mmy2'].str.title()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSRP_dict = msrp_data.groupby(['Make', 'Model', 'Year', 'mmy', 'mmy2'])['MSRP'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSRP_dict.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file = 'data'\n",
    "new_file = 'msrp.csv'\n",
    "new_file_name_path = os.path.join(basepath,sub_file, new_file)\n",
    "msrp_data.to_csv(new_file_name_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find State data\n",
    "We can use geographic data to find other data we need. Two things that effect prices is the amount of money people make so let's include that\n",
    "Similarly car prices are often influenced by gas prices so we will include that as well\n",
    "\n",
    "Gas prices come from AAA which shows the daily average gas prices for a state. They do not store this data so the data was scraped from the next nearest date availble from the scraping of the craiglist data which was Feb 22\n",
    "https://web.archive.org/web/20200226222808/https://gasprices.aaa.com/state-gas-price-averages/\n",
    "\n",
    "This data for household income was from https://dqydj.com/average-income-by-state-median-top-percentiles/ a place that holds financial data and that is reccomend by theWSJ and the NYT. This has already been done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging Auto and MSRP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create Dictionaries to map with\n",
    "\n",
    "First the MMY\n",
    "* automobiles models are referenced by its a) manufacture b) model name, and C) model year (otherwise known as the MMY)\n",
    "* Each mmy has an average MSRP to go with it and a specific groups of catehories can be in \n",
    "    * (ex a camry can be sedan or hatchback it can't be a pickup)\n",
    "* We will therefore use the MMYas a key to map the MSRP and category to the main data set\n",
    "\n",
    "Issue: Model names of automobile\n",
    "* The trim of a model is referencing a set of additional features comes with (example Camry Lux has leather seats)\n",
    "* The trim changes the price of a vehicle\n",
    "* Therefore sometimes sellers will add the trim after the model design to give a better idea of what they are selling\n",
    "* This leads to the model name not being consistent across tables.\n",
    "\n",
    "Fixing the Issue\n",
    "* To ensure better mapping we will therefore use 2 keys \n",
    "* The first key will match the full name of the model for the mmy\n",
    "* The second key will match the first word name of the model for the mmy\n",
    "* We will try to map to first the key if a map isn't reached we use the results from the second key.\n",
    "* both keys include the manufacturer and model year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionaries to map with\n",
    "\n",
    "\n",
    "#create mmy key2 for autos\n",
    "auto_data['model2']= auto_data['model'].str.split(' ').str[0]\n",
    "auto_data['mmy_a2']=auto_data['manufacturer']+' '+auto_data['model2']+ ' '+ auto_data['year'].astype(int).astype(str)\n",
    "\n",
    "#auto_dictionary\n",
    "auto_dict= auto_data[['manufacturer', 'model', 'year', 'mmy_a', 'mmy_a2']].drop_duplicates(keep='last')\n",
    "\n",
    "#MSRP_dictionary_1 has only the first key\n",
    "MSRP_dict1 = MSRP_dict[['mmy', 'MSRP']]\n",
    "\n",
    "#MSRP_dictionary_2 has only the second key\n",
    "MSRP_dict2= MSRP_dict[['mmy2', 'MSRP']]\n",
    "MSRP_dict2['MSRP_vague'] = MSRP_dict2['MSRP']\n",
    "MSRP_dict2= MSRP_dict2.drop(columns= ['MSRP'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge auto_dict and MSRP dictionary 1 into df1 using the 1st key\n",
    "df1= pd.merge(auto_dict, MSRP_dict1, left_on='mmy_a', right_on= 'mmy', how='outer' )\n",
    "#Merge dictionary3 and MSRP dictionary 2 into auto_msrp_dictionary using the second key\n",
    "a_m_dict= pd.merge(df1, MSRP_dict2, left_on='mmy_a2', right_on= 'mmy2', how='outer' )\n",
    "\n",
    "#fill any null results in MSRP from first key map by results in second key\n",
    "a_m_dict['MSRP']=a_m_dict['MSRP'].fillna(a_m_dict['MSRP_vague']) \n",
    "\n",
    "#drop duplicates\n",
    "a_m_dict= a_m_dict[['mmy_a', 'mmy', 'mmy2','MSRP']].drop_duplicates(subset=['mmy_a', 'mmy', 'mmy2'], keep= 'last' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cat_dictionary_1 has only the first key\n",
    "cat_dict= cat_data[['Make', 'Model', 'Year', 'mmy', 'mmy2','Category']].drop_duplicates(subset=['Make', 'Model', 'Year', 'mmy', 'mmy2'], keep='last')\n",
    "cat_dict1 = cat_dict[['mmy', 'Category']]\n",
    "\n",
    "\n",
    "#Cat_dictionary_2 has only the second key\n",
    "cat_dict2= cat_dict[['mmy2', 'Category']]\n",
    "cat_dict2['Category_vague'] = cat_dict2['Category']\n",
    "cat_dict2= cat_dict2.drop(columns= ['Category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge auto dictionary and cat dictionary 1 into dictionary into df1 using the first key\n",
    "df1= pd.merge(auto_dict, cat_dict1, left_on='mmy_a', right_on= 'mmy', how='outer' )\n",
    "\n",
    "#Merge dictionary3 and cat dictionary 2 into auto_cat_dictionary using the second key\n",
    "a_c_dict= pd.merge(df1, cat_dict2, left_on='mmy_a2', right_on= 'mmy2', how='outer' )\n",
    "\n",
    "#Fill in blanks in the Category left from the first key with results from the second key (if any)\n",
    "a_c_dict['Category']=a_c_dict['Category'].fillna(a_c_dict['Category_vague']) \n",
    "\n",
    "\n",
    "#drop duplicates \n",
    "a_c_dict= a_c_dict[['mmy_a', 'mmy', 'mmy2','Category']].drop_duplicates(subset=['mmy_a', 'mmy', 'mmy2'], keep= 'last' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a_c_dict.info())\n",
    "print(a_m_dict.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Cat and MSRP\n",
    "#We still have two keys mmy_a and mmy\n",
    "\n",
    "a_m_dict_1= a_m_dict[['mmy_a', 'MSRP']].dropna()\n",
    "a_m_dict_2= a_m_dict[['mmy', 'MSRP']].dropna()\n",
    "a_m_dict_3= a_m_dict[['mmy2', 'MSRP']].dropna()\n",
    "\n",
    "a_m_dict_2= a_m_dict_2.rename(columns={\"MSRP\": \"MSRP_2\"})\n",
    "a_m_dict_3= a_m_dict_3.rename(columns={\"MSRP\": \"MSRP_3\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge a_c and a_m dict on first key\n",
    "df1= pd.merge(a_c_dict, a_m_dict_1, on='mmy_a', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge a_c and a_m dict on second key\n",
    "df2 = pd.merge(df1,  a_m_dict_2, left_on='mmy', right_on='mmy', how='outer' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge a_c and a_m dict on third key\n",
    "a_m_c_dict = pd.merge(df2,  a_m_dict_3, on='mmy2', how='outer' )\n",
    "a_m_c_dict.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in blanks in the MSRP left from 1st key with results from the 2nd and 3rd key (if any)\n",
    "a_m_c_dict['MSRP']=a_m_c_dict['MSRP'].fillna(a_m_c_dict['MSRP_2']) \n",
    "a_m_c_dict['MSRP']=a_m_c_dict['MSRP'].fillna(a_m_c_dict['MSRP_3']) \n",
    "\n",
    "#drop duplicates\n",
    "a_m_c_dict= a_m_c_dict[['mmy_a', 'MSRP', 'Category']].drop_duplicates(subset=['mmy_a'], keep= 'last' )\n",
    "\n",
    "#drop if missing 'mmy_a'\n",
    "a_m_c_dict = a_m_c_dict[a_m_c_dict['mmy_a'].notna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Dictionaries with auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z= pd.merge(auto_data, a_m_c_dict, on='mmy_a', how='left')\n",
    "z.info()\n",
    "auto_data= z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add State Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#get_state_data\n",
    "state_file ='state_data.csv'\n",
    "\n",
    "print('p')\n",
    "file_name_path = os.path.join(basepath, 'data', state_file)\n",
    "state_data= pd.read_csv(file_name_path)\n",
    "state_data.info()\n",
    "#Open data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_with_state_data\n",
    "auto_data_m= auto_data.merge(state_data, left_on='state', right_on='State_Code')\n",
    "auto_data_m.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data.select_dtypes(exclude=['int64', 'float']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auto_data['mm']=auto_data['manufacturer']+' '+auto_data['model']\n",
    "auto_data['mm2']=auto_data['manufacturer']+' '+auto_data['model2']\n",
    "\n",
    "\n",
    "       \n",
    "auto_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= auto_data\n",
    "auto_data['drive1']= auto_data['drive'].fillna('')\n",
    "auto_data['cylinders1']= auto_data['cylinders'].fillna('')\n",
    "\n",
    "\n",
    "#Cylinders and drive are closely related so first class on that\n",
    "#data explaining that is auto_3_eda\n",
    "df['cylinders'] = df.groupby(['mmy_a', 'drive1'], sort=False)['cylinders'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "df['drive'] = df.groupby(['mmy_a', 'cylinders1' ], sort=False)['drive'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "\n",
    "df['cylinders'] = df.groupby(['mmy_a'], sort=False)['cylinders'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "df['drive'] = df.groupby(['mmy_a'], sort=False)['drive'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "\n",
    "df['cylinders'] = df.groupby(['model'], sort=False)['cylinders'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "df['drive'] = df.groupby(['model'], sort=False)['drive'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#on size\n",
    "df['size'] = df.groupby(['mmy_a', 'drive1'], sort=False)['size'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "df['size'] = df.groupby(['mmy_a', 'cylinders1' ], sort=False)['size'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "df['size'] = df.groupby(['mmy_a'], sort=False)['size'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "df['size'] = df.groupby(['model', 'drive1'], sort=False)['size'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "df['size'] = df.groupby(['model', 'cylinders1' ], sort=False)['size'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "df['size'] = df.groupby(['model'], sort=False)['size'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why didn't this work?? \n",
    "\n",
    "\"\"\"\n",
    "q1 = auto_data\n",
    "ts_vars = ['mmy_a', 'mm_', 'mm2', 'manufacturer']\n",
    "ls_vars = ['drive', 'cylinders', 'size', 'tranmission', 'type', 'MSRP', 'Category']\n",
    "\n",
    "\n",
    "for ts_var in ts_vars\n",
    "    ts_list = q1[ts_var].unique().tolist()\n",
    "    for ts in ts_list:\n",
    "        dfelse = q1[q1[ts_var]== ts]\n",
    "        for ls_var in ls_vars:\n",
    "            try:\n",
    "                x = dfelse[ls_var].mode()[0]\n",
    "                q1.loc[(q1['mmy_a']== mmy_a_l) & (q1[ls_var].isna()), ls_var ] = x\n",
    "            except: \n",
    "                pass\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q1 = auto_data\n",
    "\n",
    "for mmy_a_l in mmy_a_ls:\n",
    "    dfelse = q1[q1['mmy_a']== mmy_a_l]\n",
    "    for ls_var in ls_vars:\n",
    "        try:\n",
    "            x = dfelse[ls_var].mode()[0]\n",
    "            q1.loc[(q1['mmy_a']== mmy_a_l) & (q1[ls_var].isna()), ls_var ] = x\n",
    "        except: \n",
    "            pass\n",
    "q1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "    \n",
    "tes = z2[z2['manufacturer'] == 'Tesla']\n",
    "tes.drive.mode()[0]\n",
    "print(tes.count())\n",
    "z2.loc[(z2.manufacturer == 'Tesla') & (z2['drive'].isna()), 'drive'] = 'rwd' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mm\n",
    "mm2\n",
    "\n",
    "tes = z2[z2.manufacturer == 'Tesla']\n",
    "tes.drive.mode()[0]\n",
    "print(tes.count())\n",
    "z2.loc[(z2.manufacturer == 'Tesla') & (z2['drive'].isna()), 'drive'] = 'rwd' \n",
    "\n",
    ".unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#on Category\n",
    "df['Category1'] = df['Category']\n",
    "df['Category1'] = df.groupby(['mm'], sort=False)['Category1'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "df['Category'] = df.groupby(['mm2'], sort=False)['Category1'].apply(lambda x: x.fillna(next(iter(x.mode()), np.nan)))\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dum_cols= ['drive', 'size', 'type', 'transmission', 'Category']\n",
    "#lead_cols = ['mmy_a', 'mm', 'mm2']\n",
    "#auto_data1 = auto_data\n",
    "\n",
    "#for lead_col in lead_cols:\n",
    "#    for dum_col in dum_cols:\n",
    "#        auto_data1[dum_col] = auto_data1.groupby([lead_col], sort=False)[dum_col].apply(lambda x: x.fillna(x.mode().iloc[0]))\n",
    "#auto_data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data1= df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute long_lat from state\n",
    "auto_data1['fuel'] = auto_data1['fuel'].fillna(auto_data['fuel'].mode()[0])\n",
    "auto_data1['lat'] = auto_data1.groupby(['state'], sort=False)['lat'].apply(lambda x: x.fillna(x.mode().iloc[0]))\n",
    "auto_data['long'] = auto_data1.groupby(['state'], sort=False)['long'].apply(lambda x: x.fillna(x.mode().iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data1= auto_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data['Avg_Mileage'] = auto_data['odometer']/ auto_data['age']\n",
    "auto_data['resid'] = auto_data['MSRP']/ auto_data['price']\n",
    "auto_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data = auto_data.drop(columns=['mmy_a', 'mmy_a2', 'mm', 'mm2', 'model2', 'cylinders1', 'drive1' ], axis=1 )\n",
    "\n",
    "auto_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "\n",
    "new_file = 'auto_data_merged_cl.csv'\n",
    "new_file_name_path = os.path.join(basepath,sub_file, new_file)\n",
    "auto_data.to_csv(new_file_name_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data getting rid of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of file names: filenames\n",
    "file ='auto_data_merged_cl.csv'\n",
    "sub_file = 'data'\n",
    "file_name_path = os.path.join(basepath, sub_file, file)\n",
    "auto_data= pd.read_csv(file_name_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data.describe(percentiles= [0.05, .25, .5, .75, 0.95,  0.99, 0.995] )\n",
    "# Rule if the maximum number of the 98the percentile and the 99th percentile differ by larger than 1 std then you elimate those numbers\n",
    "# Similarly elimiate if 99th percentile and maximum is one standard deviation away elimate to 99th percentile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rule if the maximum number of the 98, 99 percentile differ by larger than 1 std you investigate\n",
    "    * The results of this investigation is in price, odometer and average mileage\n",
    "* Similarly we check if adding 98 percentile + 1std is worth doing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate price \n",
    "price = auto_data.price\n",
    "sns.boxplot(price)\n",
    "plt.title(\"Distribution of the price before deletion\")\n",
    "plt.show()\n",
    "#so many outliers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We normally would vehicles over the 99th percentile to get rid of outliers but as this our explanatory variable\n",
    "# so we eliminate vehicle's whose price is double the current 99th percentile of price\n",
    "auto_data= auto_data[auto_data.price< 100000 ]\n",
    "price = auto_data.price\n",
    "sns.boxplot(price)\n",
    "plt.title(\"Distribution of the price after deletion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate Mileage\n",
    "miles = auto_data.odometer\n",
    "sns.boxplot(miles)\n",
    "plt.title(\"Distribution of mileage on vehicles before deletion\")\n",
    "plt.show()\n",
    "#so many outliers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We eliminate vehicles over the 99th percentile\n",
    "auto_data= auto_data[auto_data.odometer< 200000 ]\n",
    "miles = auto_data.odometer\n",
    "sns.boxplot(miles)\n",
    "plt.title(\"Distribution of mileage on vehicles after deletion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate Average Mileage\n",
    "# Average Milage is the mileage divided by the age let's see that relationship \n",
    "g = sns.jointplot(x= 'age', y='odometer',data=auto_data , kind='kde')\n",
    "_ = g.title = 'Age vs Mileage'\n",
    "#We van see clustered Non Linear replationsip between age and mileage and some pretty clear boundries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate Average Mileage using boxplot\n",
    "avg_miles = auto_data.Avg_Mileage\n",
    "sns.boxplot(avg_miles)\n",
    "plt.title(\"Distribution of avg mileage per year on vehicles before deletion\")\n",
    "plt.show()\n",
    "#so many outliers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We eliminate vehicles over the 99th percentile\n",
    "auto_data= auto_data[auto_data.Avg_Mileage< 60000]\n",
    "avg_miles = auto_data.Avg_Mileage\n",
    "sns.boxplot(avg_miles)\n",
    "plt.title(\"Distribution of avg mileage per year on vehicles after deletion\")\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Cylinders\n",
    "#Investigate cylinders\n",
    "cylinders = auto_data.cylinders\n",
    "sns.boxplot(cylinders)\n",
    "plt.title(\"Distribution of avg mileage per year on vehicles before deletion\")\n",
    "plt.show()\n",
    "cylinders.value_counts()\n",
    "#We eliminate 12 as it only has one value \n",
    "auto_data =  auto_data[auto_data['cylinders']<12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is an interesting relationship between cylinders, and drive\n",
    "size_type_table = pd.crosstab(index=auto_data[\"drive\"], columns=auto_data[\"cylinders\"])\n",
    "plt.figure(figsize=(14,12))\n",
    "size_type_table.plot(kind=\"bar\",  figsize=(8,8), stacked=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is an interesting relationship between cylinders, drive, mileage and price\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.catplot(x=\"cylinders\", y=\"price\", hue='drive', kind='point', data=auto_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(x= 'odometer', y='price', data=auto_data,  col=\"cylinders\", hue='drive', col_wrap=3, scatter_kws={\"alpha\":.6, \"s\": 5})\n",
    "g = (g.set( ylim=(0, 100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More visuals regarding the distributions are in eda3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Category/ type\n",
    "* Category refers to the type of design of the vehicle ( examplesedan, hatchback), and it is something people take into consideration while buying a vehicle\n",
    "* However the naming convention for this is non standard across manufactuers (\"pickup\" vs \"truck\" refers to the same type from Ford and GM respectively)\n",
    "* Considering there are many users on Craigslist even though type is often in the description of a vehicle sold the naming convention would not be consistent from one user description to the next. \n",
    "* The type would not  use consistent rules through out it. To fix this we used a third party (see above merge.\n",
    "* While the third party gave us a map for some of the vehicle it did not give us all the vehicles.\n",
    "* Further the third party when it did merge often gave us a group of categories the vehicle could belonged to which needs to be narrowed down what is specific for that vehicle\n",
    "* We are going to try to use the third party's categories and what's in the user description to go for type\n",
    "* For the merge's we have we are going to check if the category within the description matches what the third party gave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean Category iin second option\n",
    "auto_data['Category2']= auto_data['Category'].str.strip().str.split(', ').str[0]\n",
    "\n",
    "\n",
    "#see what Categories are listed by third party\n",
    "print(auto_data.Category.unique())\n",
    "\n",
    "#Using the results from Category see  if a category is mentioned in the craig's list description\n",
    "cat_list= ['Sedan' , 'Convertible' , 'Coupe' , 'Hatchback' , 'Pickup' , 'Wagon',  'Van' ,'Minivan',  'SUV']\n",
    "auto_data['cat_tiki']= ''\n",
    "\n",
    "# Find if Category is description saveas cat_tiki\n",
    "for cat in cat_list:\n",
    "    auto_data['cat_tiki']=np.where(auto_data.description.str.contains(cat , case=False, na=False), cat, '')\n",
    "auto_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We aregoing to create a column Category Cleaned which has the imputed values for Category\n",
    "\n",
    "#We need this to iterate\n",
    "auto_data['cat_tiki_check0']= ''\n",
    "auto_data['cat_tiki_check1']= ''\n",
    "auto_data['cat_tiki_check2']= ''\n",
    "auto_data['Category_Cleaned']= auto_data['Category2']\n",
    "auto_data['cat_tiki_cleaned1']= auto_data['Category2']\n",
    "auto_data['Category3'] =  auto_data['Category'].fillna('') \n",
    "auto_data['type3'] =  auto_data['type'].fillna('') \n",
    "auto_data['type4'] =  auto_data['type']\n",
    "print(auto_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#the data set came also with \"type\" which... I haven't seen the rules for so we are going to use it as a possible source.\n",
    "\n",
    "#check if vehicle type listed in description is the sameone that is in the data \n",
    "auto_data['cat_tiki_check0']= auto_data.apply(lambda x: x.cat_tiki in x.type3, axis=1)\n",
    "\n",
    "#Simiplify cat_tiki so that it makes an easier match\n",
    "auto_data.loc[auto_data['cat_tiki_check0'] == True, 'type3'] = auto_data.cat_tiki\n",
    "\n",
    "#check if vehicle type listed in description is in the third party group if there is a 3rd party group\n",
    "auto_data['cat_tiki_check1']= auto_data.apply(lambda x: x.cat_tiki in x.Category3, axis=1)\n",
    "\n",
    "#Where a Category in the description is in the third party's group of Category update values to reflect this match\n",
    "auto_data.loc[auto_data['cat_tiki_check1'] == True, 'Category_Cleaned'] = auto_data.cat_tiki\n",
    "auto_data2=auto_data\n",
    "\n",
    "print(auto_data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check if vehicle type listed in data is in categories use type that was also one of the one's listed by the third party\n",
    "auto_data['cat_tiki_check2']= auto_data.apply(lambda x: x.type3 in x.Category3, axis=1)\n",
    "\n",
    "#Switch the value in Category cleaned to the one that was included in type\n",
    "auto_data.loc[auto_data['cat_tiki_check2'] == True, 'cat_tiki_cleaned1'] = auto_data.type\n",
    "\n",
    "\n",
    "auto_data['Category_Cleaned'].fillna('cat_tiki_cleaned1',inplace = True)\n",
    "\n",
    "#get rid of variables included only for iteration\n",
    "auto_data= auto_data.drop(['Category3', 'type3', 'type4', 'cat_tiki_check0', 'cat_tiki_check1', 'cat_tiki_check2', 'cat_tiki_cleaned1' ], axis=1)\n",
    "\n",
    "auto_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = 'auto_data_imputed_cl.csv'\n",
    "new_file_name_path = os.path.join(basepath,sub_file, new_file)\n",
    "auto_data.to_csv(new_file_name_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Categorical Variables\n",
    "* We want to reduce the number of unnecessary variables. Bellow are some of the reasons:\n",
    "* We Eliminate categorical variables that are included in another variable (aka intercorrelation)\n",
    "    * example keeping Category_cleaned  means we can get rid of Category, Category1, type, and cat_tiki\n",
    "* We Eliminate categorical variables that are either unique to each vehicle (not counding id) or have too many categories within it because that would create too many dummy variables \n",
    "    * Descriptions and Vins are unique to each vehicle we can eliminate both variables\n",
    "    * There are over 400 regions while this could be an interesting study we don't need it. It might be something to look into for another study\n",
    "    * There are over 10000 models and while models are not completely unique to each vehicle considering we are trying to determine the asking price when we don't know the value of the model it goes around the point of the study. However again it could be something to look into.\n",
    "* We eliminate variables that have only one value as the don't add anything to the model\n",
    "    * title status\n",
    "*  We have state data, as well as  long and lat therefore we get rid of the 50 state dummy variables this could be re included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data.select_dtypes(exclude=['int64', 'float']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop variables due to intercorrelation to Category Cleaned\n",
    "\n",
    "z2= auto_data.drop(columns=['Category', 'Category1', 'Category2', 'cat_tiki'] )\n",
    "\n",
    "#drop categorical variables unique to each entry and/or having over 100 categories within\n",
    "z2 =z2.drop(columns= ['region', 'model', 'description', 'vin' ], axis=1)\n",
    "\n",
    "#drop state due to combo of intercorrelation and too many variables\n",
    "z2 =z2.drop(columns= ['state'], axis=1)\n",
    "\n",
    "#drop to having only 1 value\n",
    "z2 =z2.drop(columns= ['title_status'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = z2[z2.manufacturer == 'Tesla']\n",
    "tes.drive.mode()[0]\n",
    "print(tes.count())\n",
    "z2.loc[(z2.manufacturer == 'Tesla') & (z2['drive'].isna()), 'drive'] = 'rwd' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tes.count())\n",
    "z2.loc[(z2.manufacturer == 'Tesla') & (z2['drive'].isna()), 'drive'] = 'rwd' \n",
    "z2.loc[(z2.manufacturer == 'Tesla') & (z2['size'].isna()), 'size'] = 'mid-size' \n",
    "tes = z2[z2.manufacturer == 'Tesla']\n",
    "print(tes.count())\n",
    "print(tes.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2.manufacturer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z2.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Nulls- We prefer our data to be without nulls with the exception of MSRP and resid there are few nulls so we eliminate those\n",
    "\n",
    "z2= z2.dropna(subset=['id', 'price', 'year', 'manufacturer', 'condition', 'cylinders', 'fuel',\n",
    "       'odometer', 'transmission', 'drive', 'paint_color', 'lat',\n",
    "       'long', 'age', 'Avg_Mileage', 'Category_Cleaned'])\n",
    "z2.manufacturer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2.manufacturer.value_counts()\n",
    "#drop if less than 10 in the category\n",
    "z2 = z2[z2.manufacturer != 'Ferrari']\n",
    "z2 = z2[z2.manufacturer != 'Porche']\n",
    "z2 = z2[z2.manufacturer != 'Aston-Martin']\n",
    "z2 = z2[z2.manufacturer != 'Land-Rover']\n",
    "#We are not loooking up superluxury cars aka\n",
    "z2.manufacturer.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z2.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminate variables due to intercorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot correlation\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(z2.corr(),linewidths=.1, annot=True)\n",
    "plt.yticks(rotation=0);\n",
    "plt.savefig('correlation',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing Scaler Standardization\n",
    "\n",
    "#### Standizing Data\n",
    "* We have a few methods of standardization to choose from: Robust, Minmax, Normalize and Standardize\n",
    "* We use Robust if we want to minimize impact of outliers\n",
    "* We use Normalize if we think the distrubtion within a variable needs to be normalized and/or has a normal distribution\n",
    "* We use Standardize when we think most of the variables have similar distribution and no/few dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = z2.hist(figsize=(40,22))\n",
    "#The data for price is and age are not a normal destribution so both normalize scaler is bad idea "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = z2.boxplot(grid=False, vert=False,fontsize=9)\n",
    "#No large outliers as that was cleaned out already earlier will not use Robust Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing Min Max Scaler\n",
    "* The data for price is and age are not a normal destribution we eliminate normalize scaler\n",
    "* No large outliers as that was cleaned out already earlier we eliminate  Robust Scaler\n",
    "* As we have dummy variables we can eliminate standard scaler \n",
    "* That leaves us with Min Max Zx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dummy variables\n",
    "auto_data_dummies= pd.get_dummies(z2)\n",
    "auto_data_dummies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check correlation between some dummy variables\n",
    "auto_data_cats= z2[['transmission', 'cylinders', 'fuel', 'drive', 'Category_Cleaned']]\n",
    "auto_data_dummies_small= pd.get_dummies(auto_data_cats)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(auto_data_dummies_small.corr(),linewidths=.1, annot=True)\n",
    "plt.yticks(rotation=0);\n",
    "plt.savefig('correlation',bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate Data for Test and Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#seperate X and y from each other\n",
    "X = auto_data_dummies\n",
    "print(type(X))\n",
    "y = auto_data_dummies['price']\n",
    "\n",
    "X =X.drop(columns=['price', 'id'], axis=1)\n",
    "X.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#split train, trial and vals\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size= 0.25, random_state=42)\n",
    "\n",
    "X_train.info()\n",
    "\n",
    "# Create val and train sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 3/11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data vased on previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize numeric variables\n",
    "\n",
    "#create scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler_sc = StandardScaler()\n",
    "\n",
    "#Scale numeric data1\n",
    "X_train_mm = scaler.fit_transform(X_train)\n",
    "X_val_mm = scaler.transform(X_val)\n",
    "X_test_mm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.0001, solver='cholesky')\n",
    " class sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n",
    "\n",
    "#Instantiate and fit a ridge reg to the min_max scaled data\n",
    "ridge_scaled = ridge.fit(X_train_mm, y_train)\n",
    "\n",
    "\n",
    "# Instantiate and fit ridge reg classifier to the unscaled data\n",
    "ridge_unscaled = ridge.fit(X_train, y_train)\n",
    "\n",
    "y_pred_mm = ridge_scaled.predict(X_val_mm)\n",
    "y_pred = ridge_unscaled.predict(X_val)\n",
    "\n",
    "# Compute and print metrics\n",
    "print('Accuracy with Min Max Scaling: {}'.format(ridge_scaled.score(X_val_mm, y_val)))\n",
    "print('Accuracy without Scaling: {}'.format(ridge_unscaled.score(X_val, y_val)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing different Ridge Regressions\n",
    "params_Ridge = {'alpha': [0.1,0.01,0.001,0.0001] , \"fit_intercept\": [True, False], \"solver\": ['svd', 'cholesky', 'lsqr']}\n",
    "ridge_reg = Ridge()\n",
    "Ridge_GS = GridSearchCV(ridge_reg, param_grid=params_Ridge, cv=5, verbose=8)\n",
    "\n",
    "Ridge_GS.fit(X_train_mm, y_train)\n",
    "print('For non scaled data using Ridge Regression')\n",
    "print('Best Score: {}'.format(Ridge_GS.best_score_))\n",
    "print('Best estimator: {}'.format(Ridge_GS.best_estimator_))\n",
    "print('Best parameters: {}'.format(Ridge_GS.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridgeregression = Ridge(random_state=42, **Ridge_GS.best_params_)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "all_accuracies = cross_val_score(estimator=Ridgeregression, X=X_train_mm, y=y_train, cv=5)\n",
    "print(all_accuracies)\n",
    "\n",
    "Ridgeregression.fit(X_train_mm, y_train)\n",
    "ridge_score= Ridgeregression.score(X_val_mm, y_val)\n",
    "\n",
    "print('Ridge using outside test_data : {}'.format(ridge_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Cs = [0.0001, 0.001, 0.005, 0.1,  1, 2, 10, 100, 1000]\n",
    "clf_scores =[]\n",
    "clf_C_dict = {}\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "# Fit the model on the trainng data.\n",
    "\n",
    "param_grid = {'C': Cs}\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(clf, param_grid, cv=5, verbose=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it to the data\n",
    "logreg_cv.fit(X_train_mm,y_train)\n",
    "\n",
    "\n",
    "\n",
    "print('For Min Max scaled data using Logistic  Regression')\n",
    "print('Best Score: {}'.format(logreg_cv.best_score_))\n",
    "print('Best estimator: {}'.format(logreg_cv.best_estimator_))\n",
    "print('Best parameters: {}'.format(logreg_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LogisticRegression_t = clf(random_state=42, **logreg_cv.best_params_)\n",
    "all_accuracies_lg = cross_val_score(estimator= LogisticRegression_t, X=X_train_mm, y=y_train, cv=5)\n",
    "print(all_accuracies_lg)\n",
    "\n",
    "LogisticRegression_t.fit(X_train_mm, y_train)\n",
    "LogisticRegression_t.score(X_val_mm, y_val)\n",
    "\n",
    "\n",
    "log_score= LogisticRegression_t.score(X_val_mm, y_val)\n",
    "print('Logistic using outside test_data : {}'.format(log_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()\n",
    "parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n",
    "grid = GridSearchCV(model,parameters, cv=None)\n",
    "grid.fit(X_train, y_train)\n",
    "print \"r2 / variance : \", grid.best_score_\n",
    "print(\"Residual sum of squares: %.2f\"\n",
    "              % np.mean((grid.predict(X_test) - y_test) ** 2))\n",
    "\n",
    "print('For Min Max scaled data using Linear  Regression')\n",
    "print('Best Score: {}'.format(grid.best_score_))\n",
    "print('Best estimator: {}'.format(grid.best_estimator_))\n",
    "print('Best parameters: {}'.format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression_t = model(random_state=42, **grid.best_params_)\n",
    "all_accuracies_lin = cross_val_score(estimator= LinearRegression_t, X=X_train_mm, y=y_train, cv=5)\n",
    "print(all_accuracies_lin)\n",
    "\n",
    "LinearRegression_t.fit(X_train_mm, y_train)\n",
    "LinearRegression_t.score(X_val_mm, y_val)\n",
    "\n",
    "lin_score= LinearRegression_t.score(X_val_mm, y_val)\n",
    "print('Logistic using outside test_data : {}'.format(lin_score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
