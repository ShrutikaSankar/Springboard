{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Prices of Used Cars on Craigslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "Buyer’s Choice (BC) is a smalled used car company based in CT lead by CEO Dapper Dan. BC's usual process for deciding to buy vehicles from the warehouse is to determine what the likely price would be if the vehicle was sold in the store (BC Price). If the BC Price is over $5k greater than the warehouse price they buy the vehicle. \n",
    "\n",
    "However with the advent of Covid people just are not coming into the store. So they have switched to selling their cars online. These prices often differ from the prices they would normally get at the store.\n",
    "\n",
    "The company was recently been given the option to buy 30 of the 2018 Tesla Model 3’s at $ 30K. They want to know if they should take the deal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem statement \n",
    "Should used car dealership Buyer’s Choice buy 30 gently used 2018 Tesla 3 at the price of \\\\$ 30K? To answer this question we need to predict the price of selling a 2018 Tesla 3 online. If it is above $ 30K we should buy the vehices. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data\n",
    "\n",
    "To determine online prices we look at the price of used vehicles on Craigslist. The dataset we used was created by Austin Reese January 2020 from scraping data from  \"every used vehicle entry within the United States\" on Craigslist on Feb 2020. Besides price this data includes other this listed on the used vehicle entry like mileage, the model and condition of the vehicle. Link: https://www.kaggle.com/austinreese/craigslist-carstrucks-data\n",
    "\n",
    "Four other supplementary databases were used:\n",
    "\n",
    "1) For vehicle model data we used data from from back4app an open data source that uses categories vehicle models based on US department of transportation. These categories are grouping such as \"convertible\" or \"pickup\".\n",
    "Website: https://www.back4app.com/database/back4app/car-make-model-dataset\n",
    "\n",
    "2) We also tried to find vehicle MSRP (aka price the market price listed by companies when the vehicle was new). We used data scraped from Edmunds (One of the most well known reviews of automobiles and their prices in the USA). This data was scraped, compiled and put on kaggle by CopperUnion in 2016 \"https://www.kaggle.com/CooperUnion/cardataset\"\n",
    "\n",
    "3) State Gas prices  come from AAA which shows the daily average gas prices for a state. They do not store this data so the data was scraped from the next nearest date available from the scraping of the Craiglist data which was Feb 22 https://web.archive.org/web/20200226222808/https://gasprices.aaa.com/state-gas-price-averages/\n",
    "\n",
    "\n",
    "4) This data for household income by state was from https://dqydj.com/average-income-by-state-median-top-percentiles/ a place that holds financial data and that is recommend by the WSJ and the NYT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning- Data Cleaning rules\n",
    "\n",
    "We want to use data that will help us predict the price of a good condition vehicle. Therefore we are not looking at million dollar superluxury cars nor are we looking at vehicle that are being sold for scraps or as junkers.\n",
    "\n",
    "* Since we are trying to predict the price of vehicles when we know the vehicle's Make, Model, Model Year, Mileage and condition we eliminate from the data set vehicles where price, make, model, model year, mileage and condition are not listed.\n",
    "* This study is not looking at Super luxury cars/ antique cars: \n",
    "    * None of the vehicles should have a price greater than half a million\n",
    "    * If MSRP is listed none of the vehicles should have an MSRP greater than a million\n",
    "* This study is not looking at motorbikes or busses \n",
    "    * We don't need Harley Davidson’s or Hennessey’s \n",
    "* This study is not looking at junker cars so we do not need vehicles:\n",
    "    * Priced at 1000 dollars or less\n",
    "    * That have over a half million miles on them\n",
    "    * In poor condition or have a title status equal or worse than salvage\n",
    "    * Are over than 10 years old (we are also not looking at antique cars)\n",
    "* Cleaning obvious errors:\n",
    "    * We are looking at used vehicles so the vehicles age should be greater than 1 year\n",
    "    * String Cleaninging aka \"BMW\" or \"bmw\" or \"BMW   \" are all manufactured by BMW\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Link to Data Cleaning Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Manipulation- Merging Data\n",
    "\n",
    "#### 4a. Mapping state data gas prices to state where the vehicle was sold\n",
    "\n",
    "\n",
    "#### 4b. Using the MMY from each vehicle we map what category the vehicle belongs to\n",
    "\n",
    "Defining MMY\n",
    "* Automobiles models are referenced by its a) manufacture b) model name, and C) model year (otherwise known as the MMY)\n",
    "* Each mmy has an average MSRP to go with it and a specific groups of catehories can be in \n",
    "    * (ex a camry can be sedan or hatchback it can't be a pickup)\n",
    "* We will therefore use the MMYas a key to map the MSRP and category to the main data set\n",
    "\n",
    "Issue: Model names of automobile\n",
    "* The trim of a model is referencing a set of additional features comes with (example Camry Lux has leather seats)\n",
    "* The trim changes the price of a vehicle\n",
    "* Therefore sometimes sellers will add the trim after the model design to give a better idea of what they are selling\n",
    "* This leads to the model name not being consistent across tables.\n",
    "\n",
    "Fixing the Issue\n",
    "* To ensure better mapping we will therefore use 2 keys \n",
    "* The first key will match the full name of the model for the mmy\n",
    "* The second key will match the first word name of the model for the mmy\n",
    "* We will try to map to first the key if a map isn't reached we use the results from the second key.\n",
    "* both keys include the manufacturer and model year    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Link to Data Manipulation Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Imputation\n",
    "\n",
    "When looking at the vehicles we note that there are lot of nulls about the vehicles features (example: the number of cylinders, drive, transmission, size, and type of vehicle). When buying a vehicle these are things you would usually ask the seller because it does effect the price of the vehicle (a vehicle with more cylinders usually is of higher value than one is of less). \n",
    "\n",
    "While these features can differ within a make (aka brand here listed as manufacturer) model, and model year (MMY) of a vehicle we can narrow down what the feature should be given the other features of the vehicle. For example: a  2016 Jaguar F-Type  can be all wheel drive or all-wheel drive ,  be a coupe or convertible, be automatic or manual transmission. You would think there be at least 8 configurations of the F-type.   However only a certain grouping of configurations (called Trims) that these vehicle are sold as.  There are actually on 6 configurations for the 2016 Jaguar F-Type because you can only get a manual transmission with rear wheel drive. \n",
    "\n",
    "If we don't have enough data to impute the vehicle given MMY and the other features we can determine the feature given its MMY. This is because A) There is usually a most popular trim for the MMY of a vehicle. B) Some features do not differ within MMY for example the 2017 Honda Accord only has 4 cylinders. \n",
    "\n",
    "If MMY does not give us a value we can look at just the make and model. The most likely configuration for model does change for each model year. However the trims of these features only change for every redesign of the model which does not happen every model year. So after looking a MMY we look at make model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Link to Data Imputation Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6a EDA- Removing Outliers\n",
    "\n",
    "We do not want any outliers in our dataset before we make any calculations. That said:\n",
    "1) We already eliminated a lot of the ouliers due to the common sense cleaning rules.\n",
    "2) The values for price, and mileage in the vehicle is a skew distribution with a long tail and not a normal distribution.\n",
    "\n",
    "Therefore we are conservative in our deletion of outliers.\n",
    "\n",
    "We beleive we should investigate four outliers when:\n",
    "* The 98the percentile and the 99th percentile differ by larger than 1 standard deviation. Then all values greater than the 98th percentile are likely to be outliers.\n",
    "* Similarly elimiate if 99th percentile and maximum differ by larger than 1 standard deviation. Then all values greater than the 99th percentile are likely to be outliers.\n",
    "\n",
    "The results of this investigation is that price, odometer, residual and average mileage needed to be investigaated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images of the price, odometer, residual and average mileage histographs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6b EDA- Visualizing Relationships within the dataset\n",
    "High corelation linear\n",
    "* Price , MSRP, \n",
    "* Price ,  cylinders\n",
    "* MSRP , cylinders\n",
    "* Residual,  cylinders\n",
    "decent correlation \n",
    "* Price odometer\n",
    "* price age\n",
    "* age odometer (this does not make sense this should have stronger relationship)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7a. Feature Selection: Dropping Categorical Variables\n",
    "* We want to reduce the number of unnecessary variables. Below are some of the reasons:\n",
    "* We eliminate categorical variables that are included in another variable (aka inter correlation)\n",
    "    * Example Division, State, and region. As each state has only one division. Each region can only fit in one states\n",
    "    * We eliminated type and Category as they both fit within Category_cleaned\n",
    "* We Eliminate categorical variables that are either unique to each vehicle or have too many categories within it because that would create too many dummy variables \n",
    "    * There are over 10000 models and while models are not completely unique to each vehicle considering we are trying to determine the asking price when we don't know the value of the model it goes around the point of the study. However again it could be something to look into.\n",
    "    * There are over 400 regions while this could be an interesting study we don't need it as we have division, longitude and latitude. It might be something to look into for another study\n",
    "    * There are over 50 states while this could be an interesting study we don't need it as we have division, longitude and latitude. It might be something to look into for another study.\n",
    "* We eliminate variables that have only one value as the don't add anything to the model\n",
    "    * title status\n",
    "    \n",
    "#### 7b. Feature Selection: Dropping Continous Variables\n",
    "We check correlation between our features. We do not want highly correlated variables (aka correlation whose absolute value is of over 0.8 in our data). In our investigation we only find year which is highly correlated to age. We therefore eliminated year from our variables.\n",
    "\n",
    "#### 7c. Feature Selection: Handling the nominal variables (aka cylinders)\n",
    "* Cylinders is an nominal variable. That means it is ordinal (1 cylinder < 2 cylinders) but it is non-continous (no such thing as half a cylinder). This is hard to model. We can either treat it as a categorical varible or we can treat is numerical variable.\n",
    "    * Treating cylinders as a categorical variable: Pro: No wrong assumptions Con: Loss of ordering\n",
    "    * Treating cylinders as a numerical variable: Pro:keep ordering information. It allows us to analyze the data using techniques that is familiar and easily understandable. Con: Incorrectly assumes continuosness\n",
    "* We prefered to lose information than to assume incorrectly so we changed cylinders to a categorical variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linke Spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Pre Processing the Data\n",
    "\n",
    "Befeore we can use the data to generate a model we need to make the data able to be understood by the model. In this case we scale the data and turn all our categorical variables into dummy variables. We then seperate the data into our training data and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8a. Chosing Scaler Standardization\n",
    "\n",
    "###### Standizing Data\n",
    "* We have a few methods of standardization to choose from: Robust, Minmax, Normalize and Standardize\n",
    "* We use Robust if we want to minimize impact of outliers\n",
    "* We use Normalize if we think the distrubtion within a variable needs to be normalized and/or has a normal distribution\n",
    "* We use Standardize when we think most of the variables have similar distribution and no/few dummy variables\n",
    "\n",
    "##### Choosing Min Max Scaler\n",
    "* The data for price is and age are not a normal destribution we eliminate normalize scaler\n",
    "* No large outliers as that was cleaned out already earlier we eliminate  Robust Scaler\n",
    "* As we have dummy variables we can eliminate standard scaler \n",
    "* That leaves us with Min Max scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8b. Pre Processing Feature Selection:Dummy Variables\n",
    "\n",
    "##### Turn each Categorical variable into multiple dummy variable\n",
    "* For example instead of having one variable for fuel where the value is listed(\"gas\" or \"electric\", or \"diesel\"). we create  dummy variables gas, electric and diesel.\n",
    "\n",
    "##### Eliminate variables due to intercorrelation\n",
    "\n",
    "1) Group Intercorrelation\n",
    "When we have a categorical variables and convert them into dummies we can cannot have all the dummies in the group since  the last dummy variable will be a linear combination of the other dummies. So we therefore eliminate one variable from the group.\n",
    "\n",
    "2) Eliminate variables where absolute correlation is greater 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Choosing the Correct Algorithm\n",
    "\n",
    "\n",
    "We tested 10 different regression models \n",
    "* Linear Regression\n",
    "* Linear Log Regression\n",
    "* Ridge Regression\n",
    "* Elasticnet Regression\n",
    "* Decision Tree Regression\n",
    "* Extra Tree Regression\n",
    "* Gamma Regression\n",
    "* Ridge with PCA\n",
    "* Linear with PCA\n",
    "* elasticnet with PCA\n",
    "\n",
    "We Test with Two parameterrs\n",
    "* Median Absolute Error: MAE\n",
    "* Mean Percentage Error: MPE\n",
    "\n",
    "Our dealer is using our prediction to decide if the asking price is lower than the predicted price he could get on Craig's list. The dealer is risk averse he does not want to take the deal if there is a high chance that the predicted price he is using is larger than the actual price. Therefore between two equally accurate models he would prefer if the  predictions under valued the car then over valued the car.\n",
    "\n",
    "The Median Absolute Error (MAE) is useful for just getting an overall fit of the model.It is a quick and easy way to compare how well one model does versus the other. Further it is not effected by the number of parameters as say R squared. This will give us the close prediction that we want. The closer the MAE\n",
    "\n",
    "The Median Percentage Error (MPE) tells us if our regression over estimates (positve MPE) or under estimates (negative MPE).  Between two similar models in accordance to our dealer's risk aversion we prefer a negative MPE. \n",
    "\n",
    "For More Information:\n",
    "\n",
    "https://www.dataquest.io/blog/understanding-regression-error-metrics/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include   ideas   for   further   research,  as   well   as   up   to   3 concrete   recommendations   on   how   your   client   can   use   your   findings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns= ['Algorithm', 'MAE Train', 'MAE Test',  'MAE Test std', 'MPE Train', 'MPE Test',  'MPE Test std']\n",
    "c_data =[c_linear, c_ridge, c_elastic, c_dt, c_ext, c_gr, c_pca_lin, c_pca_ridge, c_pca_elastic]\n",
    "convert_dict = {'MAE Train': int, 'MAE Test': int} \n",
    "score_tab = pd.DataFrame(c_data, columns= score_columns )\n",
    "score_tab = score_tab.astype(convert_dict) \n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "score_tab.sort_values('MAE Test')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
